---
title: "Data Science: Capstone Report"
author: "Louri Compain"
date: "2023-08-21"
output:
  pdf_document: default
  html_document: default
---

# Introduction

## The project

This document is my report for the Data Science : Capstone course on EDX. The goal of the project is to use a subset of the MovieLens dataset to develop a prediction of a movie's rates.

For reproducibility's sake, we will set the seed at 1, and download our libraries in a single place at the start of the project.
```{r}
set.seed(1) 
library(tidyverse)
library(caret)
library(caTools)
library(class)
library(rpart)
library(e1071)
library(h2o)
```


## Dataset

The first step was to prepare our environment by dowloading the libaries and reproducing the data for the exercise. Here is the code.

```{r}


options(timeout = 120)
ratings_file <- "ml-10M100K/ratings.dat"
movies_file <- "ml-10M100K/movies.dat"


ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```


This produces the two datasets we are working with, named edx and final_holdout_test. These were produced by separating the MovieLens Dataset into two. We will be using edx as our main training set, and final_holdout_test.will be used in the last step in order to calculate the RMSE.

```{r}
head(edx)
```

Here, we can see that EDX contains the following variables:

-   user Id and movieId, to identify the user and movie

-   rating, the variable we want to predict in the end

-   the timestamp of the review

-   the title of the film, with the date it was aired between parenthesis

-   the list of its genre, as a single column, separated by vertical bars

The title itself is of little use (good title choice might impact the ratings, but using the rules that govern this phenomena, if it exists at all, is quite ambitious). The date however, could help us. Films of a certain time period might receive more love from the reviewers, or maybe a certain genre might have had a "golden age" at some point.

The genre is an interesting variable, because it is the only one that tells us what is in the film proper. Its format is quite inconvenient, as it is a string that contains several pieces of information. It will need some transformations to become useful.

The timestamp is not about the movie itself, but there is a chance that we can use it. Maybe there are trends in reviews over time.

The movieId and userId might seem like random numbers, attributed to protect the identity of reviewers, or to connect multiple reviews on the same film without dragging a clunky string variable containing the title. But we might have a chance to exploit it. Since we made sure all userId and movieId in final hold-out test set are also in edx set, we might be able to deduce information about them, maybe even the mindset of the person behind a certain Id.

The rating is an ordinal variable, with categories that have an order (they are actually non-continous numerical values ). They are ranging from 0.5 to 5 with a 0.5 increment, giving a a total of 9 possible categories.

## Project goals

The main goal of this project is to create a model that can predict the ratings of movie reviews from the MovieLens dataset, and to optimise it to reduce its RMSE.

We want to aim for an RMS lower than 0.9, and preferably bellow 0.86490.

## Report plan

We will work using the following process:

- Analyse the data
- Exctract additional information from the dataset
- Analyse the new information from the dataset
- Implement small scale models (naive prediction, linear model, treemap, naive bayes and neural network)
- Chose the most promising small scale model
- Improve our candidate
- Test it on the full scale dataset
- Implement on the final_holdout_test data
# Methodology and analysis

## Initial Analysis

Let's see how the ratings are distributed
```{r}
ggplot(edx, aes(x=rating)) + geom_histogram() + ggtitle("Distribution of rating values")
```
We can see that reviewers prefer round rating rather than giving "half-stars". There also appears to be a skew toward high values.

We can also count the number of users and movies.
```{r}
n_distinct(edx$movieId) 
```

```{r}
n_distinct(edx$userId) 
```
We can see a lot of users and movies. This means that we will have difficulties "profiling" specific users and movies based n their ID to gather information.

We can also look at the timestamps to see when the movies were reviewed. Due to the values of timestamps however, this will be difficult to evaluate. According to the movieLens documentation, the timestamps "represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970" (https://files.grouplens.org/datasets/movielens/ml-10m-README.html)

```{r}
ggplot(edx, aes(x=timestamp)) + geom_histogram() + ggtitle("Distribution of timestamp values")
```

## Dataset preparation

First we want to transform our dataset to make the most of it. It will consist of extraction and scaling.

### Information extraction

There are two pieces information that might be relevant but are not directly accessible: the date and genre. We will then transform the dataset to extract these information and put them in a better format.

First, we will need to get the dates out of the title and into their own column.

```{r}
#date extraction based on https://www.kaggle.com/code/redroy44/movielens-dataset-analysis
# use regex to get the date from the title
dat_train_1 <- edx %>%
  mutate(title = str_trim(title)) %>%
  extract(title, c("title_tmp", "year"), regex = "^(.*) \\(([0-9 \\-]*)\\)$", remove = F) %>%
  mutate(year = if_else(str_length(year) > 4, as.integer(str_split(year, "-", simplify = T)[1]), as.integer(year))) %>%
  mutate(title = if_else(is.na(title_tmp), title, title_tmp)) %>%
  select(-title_tmp)  %>%
  mutate(genres = if_else(genres == "(no genres listed)", `is.na<-`(genres), genres))
```

Then, we extract the different values of the genre in the form of multiple binary dummy variables. One for each possible genre.

```{r}
# List of genres based on https://rpubs.com/outerelocarlos/MovieLens-Recommender-System and the https://files.grouplens.org/datasets/movielens/ml-10m-README.html documentation
genres <- c("Action", "Adventure", "Animation", 
            "Children", "Comedy", "Crime", 
            "Documentary", "Drama", "Fantasy", 
            "Film-Noir", "Horror", "Musical", 
            "Mystery", "Romance", "Sci-Fi", 
            "Thriller", "War", "Western")

for (genre_index in 1:length(genres)){
  dat_train_1[genres[genre_index]] <- NA
  detection <-as.integer(
    str_detect(
      dat_train_1$genre, 
      genres[genre_index]
      )
    )
  detection[is.na(detection)] <- 0
  dat_train_1[genres[genre_index]] <- detection
}

#rename Film-Noir and Sci-Fi to avoid issues later on with the "-"
names(dat_train_1)[names(dat_train_1) == "Film-Noir"] <- "Film_Noir"
names(dat_train_1)[names(dat_train_1) == "Sci-Fi"] <- "Sci_Fi"

```

Once these modifications have been applied, we can remove the title and the original genre columns, as they are now superfluous.

```{r}

dat_train_2 <- dat_train_1[,c(-5, -7)]
```

We will then try to get more informations about the movies and users. For this, we can chose several methods. A tempting option might be to treat movieId and userId as categorical variables and to create a binary dummy variable for each user and movie. This option is technically possible but would add thousands of factors to our later models, rendering them unusable.
Instead, we can elect to search for a deviation from the average rating based on the movie ID and the user ID. 

```{r}
avg <- mean(dat_train_2$rating)
#computing the bias
movie_avgs <- dat_train_2 %>%
  group_by(movieId) %>%
  summarize(bias_movie = mean(rating - avg))
user_avgs<- dat_train_2 %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(bias_user = mean(rating - avg - bias_movie))
```
Once the two set of bias have been created, we merge them into the training data and remove the numerical values of userId and movieId, wich pollute our dataset with meaningless numerical variables.
```{r}
#adding the bias to the main dataset
dat_train_3 <- dat_train_2%>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = avg + bias_movie + bias_user)

dat_train_3 <- dat_train_3[,c(-1,-2)]
```

### Scaling
Now that e have extracted information from our dataset, we can scale it, to improve the performance of future models. 
```{r}
#create the scale
scale <- preProcess(as.data.frame(dat_train_3[,c(-1)]), method=c("range"))
#apply it
dat_train_4 <- predict(scale, as.data.frame(dat_train_3))
```

As a result, we obtain dat_train_4, the training dataset we will be using for the rest of the project.  


## Post-preparation Analysis
Now that we adapted the dataset to our needs, we can analyse it further.

First, now that we have access to the dates we can plot them. Note that we use dat_train_3 since dat_train_4 is scaled.
```{r}
ggplot(dat_train_3, aes(x=year)) + geom_histogram()  + ggtitle("Distribution of rated movies over time")
```
It appears that most films in the database are movies from the nineties. The data collection starts with old films, up to the late 1910s, reaches a peak in the end of the twentieth century, and drops until the data collection is cut in 2010. Older movies might have an advatage due to having more time to gain appreciation and receive reviews.

We cam also plot the movie and user bias that we computed.
```{r}
ggplot(dat_train_3, aes(x=bias_movie)) + geom_histogram() + ggtitle("Distribution of movie bias")
```

For the movie bias, we can observe that most movies have a bias between -1 and 1. This variation can be quite significant on a 5 stars scales. Some rare movies have bias that drop to nearly -2.5.

```{r}
ggplot(dat_train_3, aes(x=bias_user)) + geom_histogram() + ggtitle("Distribution of user bias")
```
For the user bias, we can see that users that reach a bias of 1 or -1 are quite rare, but that the extremes push to 2 and -2.


```{r}
#change the genre to reflect the change from "-" to "_"
modified_genres <- c("Action", "Adventure", "Animation", 
            "Children", "Comedy", "Crime", 
            "Documentary", "Drama", "Fantasy", 
            "Film_Noir", "Horror", "Musical", 
            "Mystery", "Romance", "Sci_Fi", 
            "Thriller", "War", "Western")
#count number of films with each genre
genre_counter <- data.frame(modified_genres)
genre_counter$counter <- NA 
for (genre_index in 1:length(modified_genres)){
  genre_counter$counter[genre_index] <-sum(dat_train_4[modified_genres[genre_index]])
}
# display
ggplot(genre_counter, aes(x = modified_genres, y = counter)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  ggtitle("Distribution of genre accross the dataset") + 
  scale_x_discrete(name ="Genre") + 
  scale_y_discrete(name ="Number")
```
We can see that some genre are a lot more frequent than others.


# Model

We will now test and implement models to try to fullfill our goal of predicting ratings.

## Small scale tests
We create a smaller data set. It will be used to test different models and compare their performance and the time they need to train before we chose one.

```{r}
reduction_index <- createDataPartition(
  y = dat_train_4$rating, 
  times = 1, 
  p = 0.01, 
  list = FALSE
  )
reduced_set <- dat_train_4[reduction_index,]
print(nrow(reduced_set))
```
This set is considerably smaller, with 90002 entries instead of 9000055 and will allow us to test models and detect models that might take hours on the full set.

We then separate it between train and test sets, because we cannot use the final_holdout_test data before the end of the project.
```{r}
train_test_index <- createDataPartition(y = reduced_set$rating, times = 1, p = 0.2, list = FALSE)
reduced_dat_train <- reduced_set[-train_test_index,]
reduced_dat_test <- reduced_set[train_test_index,]
reduced_prediction_results <- reduced_dat_test[,1]
reduced_dat_test<- reduced_dat_test[,-1]
```

### Naive prediction
We start trying to predict our results with the simplest approach: averaging the ratings, and using that single value as a prediction for every review.
```{r}
avg <- mean(reduced_dat_train$rating)
RMSE_AVG <- RMSE(reduced_prediction_results, avg)

RMSE_AVG
```
We get an RMSE of 1.055387. This is our baseline, and the value we will be trying to beat with our other models. It is insufficient when compared to our RMSE goal, so we will have to improve.

### Linear Model

Our next model is a linear model. We use the lm function to produce a regression, and then round to the nearest value that match a possible category (0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5). 

```{r}
#create model
linear_regression_model_1 <- lm(
  rating ~ timestamp + year + Action + Adventure + Animation +
  Children + Comedy +Crime + Documentary + Drama + Fantasy +
  Film_Noir + Horror + Musical + Mystery + Romance + Sci_Fi + 
  Thriller + War + Western + bias_movie + bias_user + pred,
  data = reduced_dat_train)
summary(linear_regression_model_1)

#predict values
linear_regression_prediction_1 <- predict(
  linear_regression_model_1, 
  newdata = reduced_dat_test
  )
linear_regression_prediction_1 <- round(linear_regression_prediction_1*2, digits = 0)/2 # round to the closest cetegorical value

#calculate RMSE
linear_regression_RMSE_1 <- RMSE(
  linear_regression_prediction_1, 
  reduced_prediction_results
  )
linear_regression_RMSE_1
```
We have an RMSE of 0.8670271 This is considerably better than our baseline of 1.055387. According to our goals, it is accetable but not perfect, so we want to improve more.

### Treemap

We now try to compare with a treemap. 

```{r}
#create model
rpart_model_1 <- rpart(
  rating ~ timestamp + year + Action + Adventure + Animation +
  Children + Comedy +Crime + Documentary + Drama + Fantasy +
  Film_Noir + Horror + Musical + Mystery + Romance + Sci_Fi + 
  Thriller + War + Western + bias_movie + bias_user + pred
  , data = reduced_dat_train)
summary(rpart_model_1)
#predict values
rpart_prediction_1 <- predict(rpart_model_1, reduced_dat_test)
#calculate RMSE
rpart_RMSE_1 <- RMSE(rpart_prediction_1, reduced_prediction_results)
rpart_RMSE_1

```

we get an RMSE of 0.8756083. This is less interesting than the linear model.

### Naive Bayes

```{r}
#create model
naice_bayes_model_1 = naiveBayes(
  x = reduced_dat_train[-1],
  y = reduced_dat_train$rating
  )
#predict values
naice_bayes_prediction_1 = predict(
  naice_bayes_model_1, 
  newdata = reduced_dat_test
  )
#calculate RMSE
naice_bayes_RMSE_1 <- RMSE(
  as.numeric(as.character(naice_bayes_prediction_1)),
  reduced_prediction_results
  )
naice_bayes_RMSE_1

```
We get an RMSE of 1.12259. This is not interesting, and actually a lower performance than our baseline.

### Neural Network
Finally, we try to use an artificial neural network from the h20 library. This will be used in the same principle than the linear regression: we predict a numeric value, and round to the value of our nearest ordinal category.
```{r}
#Initialize h2o
h2o.init(nthreads = -1)
#create model
ann_model_1 = h2o.deeplearning(
  y = 'rating',
  training_frame = as.h2o(reduced_dat_train),
  activation = 'Rectifier',
  hidden = c(20,20),
  epochs = 10,
  train_samples_per_iteration = -2
  )
#predict values
ann_prediction_1 = h2o.predict(
  ann_model_1, 
  newdata = as.h2o(reduced_dat_test)
  )
ann_prediction_1 = as.vector(ann_prediction_1)
ann_prediction_1 <- round(
  as.numeric(as.character(ann_prediction_1))*2, 
  digits = 0
  )/2
#calculate RMSE
ann_RMSE_1 <- RMSE(ann_prediction_1, reduced_prediction_results)
ann_RMSE_1
 

```
We get an RMSE of 0.8705437. While this is an interesting result if we can improve it, it is inferior to our current champion, the linear model.

### Model selection
We now want to select the model that we will be trying to improve and extend to our entire dataset. 

The naive option gave us 1.062822
The linear model gave us 0.8670271
The treemap model gave us 0.8756083
The naive bayes gave us 1.12259
The neural network gave us 0.8705437


### Model Improvement.

The best option seems to be the linear model.
Let's try to optimize locally with interaction terms.

```{r}
#create model
linear_regression_model_2 <- lm(
  rating ~ .^2, #all values and all interaction terms
  data = reduced_dat_train)

#predict values
linear_regression_prediction_2 <- predict(
  linear_regression_model_2, 
  newdata = reduced_dat_test
  )
linear_regression_prediction_2 <- round(linear_regression_prediction_2*2, digits = 0)/2

#calculate RMSE
linear_regression_RMSE_2 <- RMSE(
  linear_regression_prediction_2, 
  reduced_prediction_results
  )
linear_regression_RMSE_2
```

We get an RMSE of  0.8648299, but the number of parameters significantly increases, ad so does the computing time. We can try to select a handful of interaction terms.

We can find the most relevant parameters with the following. 
```{r}
varImp(linear_regression_model_2, conditional=TRUE)
```

This allows us to spot one particularly important interaction: bias_movie:bias_user.

```{r}
#create model
linear_regression_model_3 <- lm(
  rating ~ timestamp + year + Action + Adventure + 
    Animation + Children + Comedy + Crime + Documentary + Drama + 
    Fantasy + Film_Noir + Horror + Musical + Mystery + Romance + 
    Sci_Fi + Thriller + War + Western + bias_movie + bias_user + bias_movie:bias_user,
  data = reduced_dat_train)

#predict values
linear_regression_prediction_3 <- predict(
  linear_regression_model_3, 
  newdata = reduced_dat_test
  )
linear_regression_prediction_3 <- round(linear_regression_prediction_3*2, digits = 0)/2

#calculate RMSE
linear_regression_RMSE_3 <- RMSE(
  linear_regression_prediction_3, 
  reduced_prediction_results
  )
linear_regression_RMSE_3
```

We get an RMSE 0.86548. We did have a small increase in RMSE, but we still have some rather good results. 


## Final Implementation

We now move on to the implementation of the model on the full dataset. 

### Final test data

We modified the training data to fit our purpose. We must now apply the same modifications to the final_holdout_test data in order. We cannot use the final_holdout_test to train, so we must use the movie_avgs and user_avgs that we got from the training data.
We will also use the scale we got from the training data, because using two different scales would be a problem.

```{r}
# use regex to get the date from the title
final_holdout_test_2 <- final_holdout_test %>%
  mutate(title = str_trim(title)) %>%
  extract(title, c("title_tmp", "year"), regex = "^(.*) \\(([0-9 \\-]*)\\)$", remove = F) %>%
  mutate(year = if_else(str_length(year) > 4, as.integer(str_split(year, "-", simplify = T)[1]), as.integer(year))) %>%
  mutate(title = if_else(is.na(title_tmp), title, title_tmp)) %>%
  select(-title_tmp)  %>%
  mutate(genres = if_else(genres == "(no genres listed)", `is.na<-`(genres), genres))


for (genre_index in 1:length(genres)){
  final_holdout_test_2[genres[genre_index]] <- NA
  final_holdout_test_2[genres[genre_index]] <- as.integer(str_detect(final_holdout_test_2$genre, genres[genre_index]))
}

#rename Film-Noir and Sci-Fi to avoid issues later on with the "-"
names(final_holdout_test_2)[names(final_holdout_test_2) == "Film-Noir"] <- "Film_Noir"
names(final_holdout_test_2)[names(final_holdout_test_2) == "Sci-Fi"] <- "Sci_Fi"

#add the biases for movie and user, from the training data
final_holdout_test_3 <- final_holdout_test_2%>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = avg + bias_movie + bias_user)

final_holdout_test_3 <- final_holdout_test_3[,c(-1,-2, -5, -7)] # remove excess columns

final_holdout_test_4 <- predict(scale, as.data.frame(final_holdout_test_3)) #set to the same scale as the training data

```

### Model implementation and optimisation
For one last test, we will separate the full training test between a training and testing group, and see if our model maintains its interest after being applied to a greater amount of data.
```{r}
train_test_index <- createDataPartition(y = dat_train_4$rating, times = 1, p = 0.2, list = FALSE)
check_dat_train <- dat_train_4[-train_test_index,]
check_dat_test <- dat_train_4[train_test_index,]
check_prediction_results <- check_dat_test[,1]
check_dat_test<- check_dat_test[,-1]

```
We apply this data to the model.
```{r}
#create model
linear_regression_model_4 <- lm(
  rating ~ timestamp + year + Action + Adventure + Animation +
  Children + Comedy +Crime + Documentary + Drama + Fantasy +
  Film_Noir + Horror + Musical + Mystery + Romance + Sci_Fi + 
  Thriller + War + Western + bias_movie + bias_user + bias_movie:bias_user,
  data = check_dat_train)

#predict values
linear_regression_prediction_4 <- predict(
  linear_regression_model_4, 
  newdata = check_dat_test
  )
linear_regression_prediction_4 <- round(linear_regression_prediction_4*2, digits = 0)/2

#calculate RMSE
linear_regression_RMSE_4 <- RMSE(
  linear_regression_prediction_4, 
  check_prediction_results
  )
linear_regression_RMSE_4
```
We get an RMSE of 0.8671712. We seem to have lost some performance, but maintain acceptable values. We can proceed to the final evaluation.


### Final model
Finally, we implement on the full training dataset and compare to our final_holdout_test and see how we fare.
```{r}
final_model <- lm(
  rating ~ timestamp + year + Action + Adventure + Animation +
  Children + Comedy +Crime + Documentary + Drama + Fantasy +
  Film_Noir + Horror + Musical + Mystery + Romance + Sci_Fi + 
  Thriller + War + Western + bias_movie + bias_user + bias_movie:bias_user,
  data= dat_train_4
  )


final_prediction <- predict(final_model, final_holdout_test_4[,-1])
final_RMSE <- RMSE(final_prediction, final_holdout_test_4[,1])
final_RMSE

```
The final RMSE is 0.8638888.

# Conclusion

The aim of this project was to predict movie ratings from the MovieLens dataset. We transformed the provided dataset to extract information from it, tested several possible models on a small scale, selected and improved the most promising one and trained it on the full scale dataset for the final prediction. This work led us to a prediction with an RMSE of 0.8638888, which fits the desired performance.

This project has limitations. A number of techniques, such as regularization, might have added some performance. Some of the methodologies dismissed on a small scale might have reacted better on the full dataset and resulted in better results. We also can notice that the methodology was skimming between regression and classification. While this produced results that fit the requested performance, a "pure" classification methodology might have been interesting to implement.

In the future, I would be interested in pursuing this project by exploring further the neural network model. The H2O library offers a very large amount of forms of options for optimization, and I am curious to see if it would be possible to explore some parameters that were used with their default configuration in the test. 

# Bibliography

My work was informed by a variety of previous works over the web. 

## The MovieLens documentation
MovieLens 10M/100k Data Set README. (n.d.). Files.grouplens.org. https://files.grouplens.org/datasets/movielens/ml-10m-README.html

## Other analysis project 
These projects were projects using the MovieLens dataset, publicly available over the web 

Bandurski, P. (2017, February 2). movieLens dataset analysis. Kaggle.com. https://www.kaggle.com/code/redroy44/movielens-dataset-analysis

Jailani, F. (2020, September 23). RPubs - Movie rating prediction in R. Api.rpubs.com. https://api.rpubs.com/fjailani/movielens

Khonje, J. (2021, June 30). RPubs - MovieLens Rating Prediction using Machine Learning Project. Rpubs.com. https://rpubs.com/Khonjeja/794882

Mineo, G. (2019, March 24). MovieLens-Rating-Prediction-Project. GitHub. https://github.com/gmineo/MovieLens-Rating-Prediction-Project

Outerelo, C. (n.d.). RPubs - MovieLens Recommender System. Rpubs.com. Retrieved November 21, 2023, from https://rpubs.com/outerelocarlos/MovieLens-Recommender-System

